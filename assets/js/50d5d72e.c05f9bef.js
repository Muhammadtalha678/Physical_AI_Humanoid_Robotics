"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[1049],{4244:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"week-13/speech-recognition","title":"Speech Recognition and Natural Language Understanding","description":"Robust speech recognition and natural language understanding (NLU) are fundamental for natural human-robot interaction. This section covers the principles, techniques, and implementation strategies for processing human speech in real-world robotic environments.","source":"@site/docs/week-13/speech-recognition.md","sourceDirName":"week-13","slug":"/week-13/speech-recognition","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-13/speech-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammadtalha678/Physical-AI-Humanoid-Robotics/edit/main/docs/docs/week-13/speech-recognition.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Speech Recognition and Natural Language Understanding"},"sidebar":"tutorialSidebar","previous":{"title":"Integrating GPT Models for Conversational AI","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-13/gpt-conversational-ai"},"next":{"title":"Multi-Modal Interaction","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-13/multi-modal-interaction"}}');var o=t(4848),s=t(8453);const a={sidebar_position:2,title:"Speech Recognition and Natural Language Understanding"},r="Speech Recognition and Natural Language Understanding",l={},c=[{value:"Fundamentals of Speech Processing",id:"fundamentals-of-speech-processing",level:2},{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:3},{value:"Acoustic Models",id:"acoustic-models",level:3},{value:"Advanced Speech Recognition",id:"advanced-speech-recognition",level:2},{value:"Online/Streaming Recognition",id:"onlinestreaming-recognition",level:3},{value:"Multi-Microphone Processing",id:"multi-microphone-processing",level:3},{value:"Natural Language Understanding (NLU)",id:"natural-language-understanding-nlu",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Context-Aware Understanding",id:"context-aware-understanding",level:3},{value:"Robustness Techniques",id:"robustness-techniques",level:2},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:3},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Integration with Robotics Systems",id:"integration-with-robotics-systems",level:2},{value:"Robot Speech Interface",id:"robot-speech-interface",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"speech-recognition-and-natural-language-understanding",children:"Speech Recognition and Natural Language Understanding"})}),"\n",(0,o.jsx)(n.p,{children:"Robust speech recognition and natural language understanding (NLU) are fundamental for natural human-robot interaction. This section covers the principles, techniques, and implementation strategies for processing human speech in real-world robotic environments."}),"\n",(0,o.jsx)(n.h2,{id:"fundamentals-of-speech-processing",children:"Fundamentals of Speech Processing"}),"\n",(0,o.jsx)(n.h3,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The speech recognition process involves multiple stages:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport librosa\nimport webrtcvad\nfrom scipy import signal\n\nclass SpeechRecognitionPipeline:\n    def __init__(self):\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness mode 2\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n    def preprocess_audio(self, audio_data):\n        """\n        Preprocess audio for speech recognition\n        """\n        # Normalize audio\n        audio_data = audio_data / np.max(np.abs(audio_data))\n\n        # Apply pre-emphasis filter\n        pre_emphasis = 0.97\n        audio_data = np.append(audio_data[0], audio_data[1:] - pre_emphasis * audio_data[:-1])\n\n        # Noise reduction\n        audio_data = self.apply_noise_reduction(audio_data)\n\n        return audio_data\n\n    def voice_activity_detection(self, audio_data):\n        """\n        Detect voice activity in audio\n        """\n        # Convert to frames\n        frames = self.frame_audio(audio_data)\n\n        # Apply VAD to each frame\n        vad_results = []\n        for frame in frames:\n            is_speech = self.vad.is_speech(frame.tobytes(), self.sample_rate)\n            vad_results.append(is_speech)\n\n        return vad_results\n\n    def frame_audio(self, audio_data):\n        """\n        Split audio into frames for VAD\n        """\n        # Pad audio to ensure proper framing\n        padding = self.frame_size - (len(audio_data) % self.frame_size)\n        padded_audio = np.pad(audio_data, (0, padding), mode=\'constant\')\n\n        # Reshape into frames\n        num_frames = len(padded_audio) // self.frame_size\n        frames = padded_audio[:num_frames * self.frame_size].reshape(num_frames, self.frame_size)\n\n        return frames.astype(np.int16)\n\n    def apply_noise_reduction(self, audio_data):\n        """\n        Apply basic noise reduction\n        """\n        # Spectral subtraction method\n        # Calculate noise profile from beginning of audio (assumed to be silence)\n        noise_profile = np.mean(np.abs(audio_data[:int(self.sample_rate * 0.5)]))\n\n        # Apply spectral subtraction\n        magnitude = np.abs(audio_data)\n        enhanced_magnitude = np.maximum(magnitude - noise_profile, 0)\n        enhanced_audio = audio_data * (enhanced_magnitude / magnitude)\n\n        return enhanced_audio\n'})}),"\n",(0,o.jsx)(n.h3,{id:"acoustic-models",children:"Acoustic Models"}),"\n",(0,o.jsx)(n.p,{children:"Understanding different acoustic model approaches:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class AcousticModelManager:\n    def __init__(self):\n        self.models = {\n            \'conformer\': self.load_conformer_model,\n            \'wavenet\': self.load_wavenet_model,\n            \'rnn_transducer\': self.load_rnnt_model\n        }\n        self.current_model = None\n\n    def load_conformer_model(self):\n        """\n        Load Conformer-based acoustic model\n        """\n        # In practice, this would load a pre-trained model\n        # Conformer models provide good accuracy with reasonable latency\n        pass\n\n    def load_wavenet_model(self):\n        """\n        Load WaveNet-based acoustic model\n        """\n        # WaveNet provides high-quality audio processing but is computationally expensive\n        pass\n\n    def load_rnnt_model(self):\n        """\n        Load RNN-Transducer model\n        """\n        # RNN-T models are good for streaming recognition\n        pass\n\n    def select_model(self, requirements):\n        """\n        Select appropriate model based on requirements\n        """\n        if requirements.get(\'low_latency\', False):\n            return self.models[\'rnnt_transducer\']()\n        elif requirements.get(\'high_accuracy\', False):\n            return self.models[\'conformer\']()\n        elif requirements.get(\'streaming\', False):\n            return self.models[\'rnnt_transducer\']()\n        else:\n            return self.models[\'conformer\']()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-speech-recognition",children:"Advanced Speech Recognition"}),"\n",(0,o.jsx)(n.h3,{id:"onlinestreaming-recognition",children:"Online/Streaming Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Real-time speech recognition for continuous interaction:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport threading\nfrom collections import deque\n\nclass StreamingSpeechRecognizer:\n    def __init__(self, model_path):\n        self.model_path = model_path\n        self.audio_buffer = deque(maxlen=16000)  # 1 second buffer at 16kHz\n        self.is_listening = False\n        self.recognition_thread = None\n        self.partial_results = []\n        self.final_results = []\n\n    def start_streaming(self):\n        """\n        Start streaming speech recognition\n        """\n        self.is_listening = True\n        self.recognition_thread = threading.Thread(target=self._process_stream)\n        self.recognition_thread.start()\n\n    def _process_stream(self):\n        """\n        Process audio stream in real-time\n        """\n        while self.is_listening:\n            if len(self.audio_buffer) >= 1600:  # Process 100ms chunks\n                chunk = list(self.audio_buffer)[:1600]\n                self.audio_buffer = deque(list(self.audio_buffer)[1600:], maxlen=16000)\n\n                # Perform recognition on chunk\n                partial_result = self.recognize_chunk(chunk)\n\n                # Add to partial results\n                self.partial_results.append(partial_result)\n\n    def recognize_chunk(self, audio_chunk):\n        """\n        Recognize a small chunk of audio\n        """\n        # Apply acoustic model to chunk\n        # This is a simplified representation\n        # In practice, this would use a streaming model\n        return self.apply_acoustic_model(audio_chunk)\n\n    def apply_acoustic_model(self, audio_chunk):\n        """\n        Apply acoustic model to audio chunk\n        """\n        # Convert to features\n        features = self.extract_features(audio_chunk)\n\n        # Apply model\n        # model_output = self.acoustic_model.predict(features)\n\n        # Decode to text\n        # text = self.decoder.decode(model_output)\n\n        # For this example, return a placeholder\n        return "partial recognition result"\n\n    def get_partial_result(self):\n        """\n        Get current partial recognition result\n        """\n        return " ".join(self.partial_results)\n\n    def stop_streaming(self):\n        """\n        Stop streaming recognition\n        """\n        self.is_listening = False\n        if self.recognition_thread:\n            self.recognition_thread.join()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"multi-microphone-processing",children:"Multi-Microphone Processing"}),"\n",(0,o.jsx)(n.p,{children:"Using multiple microphones for improved speech recognition:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\nclass MultiMicrophoneProcessor:\n    def __init__(self, num_mics=4):\n        self.num_mics = num_mics\n        self.mic_positions = self.calculate_mic_positions()\n        self.beamformer = self.initialize_beamformer()\n\n    def calculate_mic_positions(self):\n        """\n        Calculate microphone positions in array\n        """\n        # Assume a circular microphone array\n        positions = []\n        radius = 0.05  # 5cm radius\n        for i in range(self.num_mics):\n            angle = 2 * np.pi * i / self.num_mics\n            x = radius * np.cos(angle)\n            y = radius * np.sin(angle)\n            positions.append([x, y, 0])\n        return np.array(positions)\n\n    def delay_and_sum_beamforming(self, multi_channel_audio, look_direction):\n        """\n        Apply delay-and-sum beamforming\n        """\n        # Calculate time delays for target direction\n        delays = self.calculate_delays(look_direction)\n\n        # Apply delays to each channel\n        delayed_signals = []\n        for i, audio in enumerate(multi_channel_audio):\n            delayed = self.apply_delay(audio, delays[i])\n            delayed_signals.append(delayed)\n\n        # Sum all channels\n        beamformed = np.sum(delayed_signals, axis=0)\n\n        return beamformed\n\n    def calculate_delays(self, look_direction):\n        """\n        Calculate delays for beamforming in target direction\n        """\n        delays = []\n        for pos in self.mic_positions:\n            # Calculate distance to plane wave\n            distance = np.dot(pos, look_direction)\n            delay_samples = distance / 343.0 * 16000  # speed of sound = 343 m/s\n            delays.append(int(delay_samples))\n        return delays\n\n    def apply_delay(self, signal, delay_samples):\n        """\n        Apply delay to signal\n        """\n        if delay_samples > 0:\n            return np.concatenate([np.zeros(delay_samples), signal[:-delay_samples]])\n        else:\n            delay_samples = abs(delay_samples)\n            return np.concatenate([signal[delay_samples:], np.zeros(delay_samples)])\n\n    def noise_reduction(self, multi_channel_audio):\n        """\n        Apply noise reduction using multiple microphones\n        """\n        # Calculate spatial covariance matrix\n        cov_matrix = self.calculate_covariance_matrix(multi_channel_audio)\n\n        # Apply MUSIC or other spatial filtering algorithm\n        enhanced_audio = self.apply_spatial_filter(cov_matrix, multi_channel_audio)\n\n        return enhanced_audio\n\n    def calculate_covariance_matrix(self, multi_channel_audio):\n        """\n        Calculate spatial covariance matrix\n        """\n        # Convert to frequency domain\n        freq_domain = np.array([np.fft.fft(channel) for channel in multi_channel_audio])\n\n        # Calculate covariance matrix for each frequency bin\n        cov_matrix = np.zeros((self.num_mics, self.num_mics), dtype=complex)\n        for f in range(freq_domain.shape[1]):\n            freq_vec = freq_domain[:, f]\n            cov_matrix += np.outer(freq_vec, np.conj(freq_vec))\n\n        return cov_matrix / freq_domain.shape[1]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"natural-language-understanding-nlu",children:"Natural Language Understanding (NLU)"}),"\n",(0,o.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,o.jsx)(n.p,{children:"Classifying user intents from recognized speech:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport re\n\nclass IntentClassifier:\n    def __init__(self):\n        self.pipeline = Pipeline([\n            ('tfidf', TfidfVectorizer(ngram_range=(1, 2), stop_words='english')),\n            ('classifier', MultinomialNB())\n        ])\n        self.intents = {}\n        self.is_trained = False\n\n    def train(self, training_data):\n        \"\"\"\n        Train intent classifier\n        \"\"\"\n        texts = [item['text'] for item in training_data]\n        labels = [item['intent'] for item in training_data]\n\n        self.pipeline.fit(texts, labels)\n        self.is_trained = True\n\n        # Store intent definitions\n        for item in training_data:\n            if item['intent'] not in self.intents:\n                self.intents[item['intent']] = []\n            self.intents[item['intent']].append(item['text'])\n\n    def classify_intent(self, text):\n        \"\"\"\n        Classify intent of input text\n        \"\"\"\n        if not self.is_trained:\n            return {'intent': 'unknown', 'confidence': 0.0}\n\n        # Predict intent\n        predicted_intent = self.pipeline.predict([text])[0]\n        confidence = max(self.pipeline.predict_proba([text])[0])\n\n        return {\n            'intent': predicted_intent,\n            'confidence': confidence,\n            'entities': self.extract_entities(text, predicted_intent)\n        }\n\n    def extract_entities(self, text, intent):\n        \"\"\"\n        Extract entities based on intent\n        \"\"\"\n        entities = []\n\n        # Define entity patterns for different intents\n        entity_patterns = {\n            'navigation': [\n                (r'to (\\w+)', 'location'),\n                (r'go to (\\w+)', 'location'),\n                (r'navigate to (\\w+)', 'location')\n            ],\n            'manipulation': [\n                (r'pick up (\\w+)', 'object'),\n                (r'grasp (\\w+)', 'object'),\n                (r'take (\\w+)', 'object')\n            ],\n            'information': [\n                (r'what is (\\w+)', 'topic'),\n                (r'tell me about (\\w+)', 'topic'),\n                (r'explain (\\w+)', 'topic')\n            ]\n        }\n\n        if intent in entity_patterns:\n            for pattern, entity_type in entity_patterns[intent]:\n                matches = re.findall(pattern, text, re.IGNORECASE)\n                for match in matches:\n                    entities.append({\n                        'type': entity_type,\n                        'value': match,\n                        'text': match\n                    })\n\n        return entities\n"})}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-understanding",children:"Context-Aware Understanding"}),"\n",(0,o.jsx)(n.p,{children:"Understanding speech in context:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ContextAwareNLU:\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()\n        self.context_stack = []\n        self.entity_linker = EntityLinker()  # Assumed implementation\n\n    def process_utterance(self, text, context=None):\n        \"\"\"\n        Process utterance with context awareness\n        \"\"\"\n        # Add context to text for better understanding\n        contextual_text = self.add_context_to_text(text, context)\n\n        # Classify intent\n        intent_result = self.intent_classifier.classify_intent(contextual_text)\n\n        # Resolve entities in context\n        resolved_entities = self.resolve_entities_in_context(\n            intent_result['entities'], context\n        )\n\n        # Update context\n        self.update_context(intent_result, resolved_entities)\n\n        return {\n            'intent': intent_result['intent'],\n            'confidence': intent_result['confidence'],\n            'entities': resolved_entities,\n            'context': self.get_current_context()\n        }\n\n    def add_context_to_text(self, text, context):\n        \"\"\"\n        Add relevant context to text for better understanding\n        \"\"\"\n        if not context:\n            return text\n\n        # Add context-relevant information to text\n        context_text = \"\"\n        if 'previous_intent' in context:\n            context_text += f\" previously intent was {context['previous_intent']}. \"\n        if 'current_task' in context:\n            context_text += f\" current task is {context['current_task']}. \"\n        if 'available_objects' in context:\n            context_text += f\" available objects are {', '.join(context['available_objects'])}. \"\n\n        return context_text + text\n\n    def resolve_entities_in_context(self, entities, context):\n        \"\"\"\n        Resolve entities based on context\n        \"\"\"\n        resolved = []\n        for entity in entities:\n            if entity['type'] == 'location' and context:\n                # Resolve ambiguous locations\n                resolved_value = self.resolve_location(entity['value'], context)\n                resolved.append({\n                    'type': entity['type'],\n                    'value': resolved_value,\n                    'original_text': entity['text']\n                })\n            elif entity['type'] == 'object' and context:\n                # Resolve object references\n                resolved_value = self.resolve_object(entity['value'], context)\n                resolved.append({\n                    'type': entity['type'],\n                    'value': resolved_value,\n                    'original_text': entity['text']\n                })\n            else:\n                resolved.append(entity)\n\n        return resolved\n\n    def resolve_location(self, location_name, context):\n        \"\"\"\n        Resolve location reference in context\n        \"\"\"\n        if context and 'known_locations' in context:\n            # Check for partial matches or aliases\n            for known_loc in context['known_locations']:\n                if location_name.lower() in known_loc.lower() or \\\n                   known_loc.lower() in location_name.lower():\n                    return known_loc\n        return location_name\n\n    def resolve_object(self, object_name, context):\n        \"\"\"\n        Resolve object reference in context\n        \"\"\"\n        if context and 'visible_objects' in context:\n            # Check for visible objects\n            for obj in context['visible_objects']:\n                if object_name.lower() in obj.lower():\n                    return obj\n        return object_name\n\n    def update_context(self, intent_result, entities):\n        \"\"\"\n        Update conversation context based on current utterance\n        \"\"\"\n        self.context_stack.append({\n            'timestamp': self.get_current_time(),\n            'intent': intent_result['intent'],\n            'entities': entities\n        })\n\n        # Keep only recent context\n        if len(self.context_stack) > 10:\n            self.context_stack.pop(0)\n\n    def get_current_context(self):\n        \"\"\"\n        Get current context for the conversation\n        \"\"\"\n        if not self.context_stack:\n            return {}\n\n        # Return recent context\n        recent_context = self.context_stack[-3:]  # Last 3 exchanges\n        return {\n            'recent_intents': [item['intent'] for item in recent_context],\n            'recent_entities': [item['entities'] for item in recent_context],\n            'full_context': self.context_stack\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"robustness-techniques",children:"Robustness Techniques"}),"\n",(0,o.jsx)(n.h3,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,o.jsx)(n.p,{children:"Handling recognition errors gracefully:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class RobustSpeechProcessor:\n    def __init__(self):\n        self.confidence_threshold = 0.7\n        self.retry_count = 3\n        self.confirmation_required_intents = [\n            'navigation', 'manipulation', 'sensitive_action'\n        ]\n\n    def process_with_error_handling(self, audio_input):\n        \"\"\"\n        Process speech with error handling\n        \"\"\"\n        # Initial recognition\n        result = self.recognize_speech(audio_input)\n\n        # Check confidence\n        if result['confidence'] < self.confidence_threshold:\n            # Try alternative recognition approaches\n            result = self.retry_with_alternatives(audio_input)\n\n        # If still low confidence, ask for clarification\n        if result['confidence'] < self.confidence_threshold:\n            return self.request_clarification(result)\n\n        # For critical intents, request confirmation\n        if result['intent'] in self.confirmation_required_intents:\n            return self.request_confirmation(result)\n\n        return result\n\n    def retry_with_alternatives(self, audio_input):\n        \"\"\"\n        Retry recognition with different models or parameters\n        \"\"\"\n        # Try with different acoustic models\n        models_to_try = ['model1', 'model2', 'model3']\n        results = []\n\n        for model in models_to_try:\n            result = self.recognize_with_model(audio_input, model)\n            results.append(result)\n\n        # Return the result with highest confidence\n        best_result = max(results, key=lambda x: x.get('confidence', 0))\n        return best_result\n\n    def request_clarification(self, low_confidence_result):\n        \"\"\"\n        Request user to clarify their request\n        \"\"\"\n        return {\n            'action': 'request_clarification',\n            'original_result': low_confidence_result,\n            'message': \"I didn't quite understand. Could you please repeat that?\"\n        }\n\n    def request_confirmation(self, high_confidence_result):\n        \"\"\"\n        Request confirmation for critical actions\n        \"\"\"\n        intent = high_confidence_result['intent']\n        entities = high_confidence_result['entities']\n\n        if intent == 'navigation':\n            location = entities[0]['value'] if entities else 'unknown'\n            confirmation_text = f\"You want me to go to {location}. Is that correct?\"\n        elif intent == 'manipulation':\n            obj = entities[0]['value'] if entities else 'unknown object'\n            confirmation_text = f\"You want me to pick up {obj}. Is that correct?\"\n        else:\n            confirmation_text = f\"You want me to {intent}. Is that correct?\"\n\n        return {\n            'action': 'request_confirmation',\n            'original_result': high_confidence_result,\n            'confirmation_text': confirmation_text\n        }\n\n    def handle_confirmation_response(self, user_response):\n        \"\"\"\n        Handle user's confirmation response\n        \"\"\"\n        if any(word in user_response.lower() for word in ['yes', 'correct', 'right', 'okay', 'sure']):\n            # Extract and return the original action\n            return self.extract_original_action()\n        elif any(word in user_response.lower() for word in ['no', 'wrong', 'incorrect', 'cancel']):\n            return {\n                'action': 'cancel',\n                'message': \"Okay, I won't do that.\"\n            }\n        else:\n            # Unclear response, ask again\n            return {\n                'action': 'request_clarification',\n                'message': \"Please say yes to confirm or no to cancel.\"\n            }\n"})}),"\n",(0,o.jsx)(n.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,o.jsx)(n.p,{children:"Adapting speech recognition to specific domains:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class DomainAdaptiveRecognizer:\n    def __init__(self):\n        self.base_model = None\n        self.domain_models = {}\n        self.domain_vocabulary = {}\n        self.current_domain = 'general'\n\n    def set_domain(self, domain_name):\n        \"\"\"\n        Set the current domain for recognition\n        \"\"\"\n        self.current_domain = domain_name\n\n        # Load domain-specific model if available\n        if domain_name in self.domain_models:\n            self.active_model = self.domain_models[domain_name]\n        else:\n            self.active_model = self.base_model\n\n        # Update vocabulary for domain\n        if domain_name in self.domain_vocabulary:\n            self.update_recognition_vocabulary(self.domain_vocabulary[domain_name])\n\n    def add_domain_data(self, domain_name, training_data, vocabulary):\n        \"\"\"\n        Add training data and vocabulary for a domain\n        \"\"\"\n        # Fine-tune model for domain\n        domain_model = self.fine_tune_model(self.base_model, training_data)\n        self.domain_models[domain_name] = domain_model\n\n        # Store domain vocabulary\n        self.domain_vocabulary[domain_name] = vocabulary\n\n    def fine_tune_model(self, base_model, training_data):\n        \"\"\"\n        Fine-tune base model with domain data\n        \"\"\"\n        # This would typically involve transfer learning techniques\n        # For this example, we'll return the base model\n        return base_model\n\n    def update_recognition_vocabulary(self, vocabulary):\n        \"\"\"\n        Update the recognition vocabulary for better accuracy\n        \"\"\"\n        # Update language model with domain-specific vocabulary\n        # This would typically involve updating the decoding graph\n        pass\n\n    def recognize_in_domain(self, audio_input):\n        \"\"\"\n        Recognize speech in the current domain\n        \"\"\"\n        # Use domain-specific model and vocabulary\n        result = self.active_model.recognize(audio_input)\n\n        # Apply domain-specific post-processing\n        result = self.apply_domain_post_processing(result, self.current_domain)\n\n        return result\n\n    def apply_domain_post_processing(self, result, domain):\n        \"\"\"\n        Apply domain-specific post-processing to recognition result\n        \"\"\"\n        if domain == 'navigation':\n            # Correct common navigation-related misrecognitions\n            result['text'] = self.correct_navigation_terms(result['text'])\n        elif domain == 'manipulation':\n            # Correct manipulation-related terms\n            result['text'] = self.correct_manipulation_terms(result['text'])\n\n        return result\n\n    def correct_navigation_terms(self, text):\n        \"\"\"\n        Correct common navigation-related misrecognitions\n        \"\"\"\n        corrections = {\n            'kitchen': ['chicken', 'ketchen', 'citchen'],\n            'bedroom': ['bedroom', 'bed room', 'bed rum'],\n            'living room': ['living room', 'livingrum', 'living room'],\n            'bathroom': ['bathroom', 'bath room', 'bathrum']\n        }\n\n        for correct, possible in corrections.items():\n            for wrong in possible:\n                if wrong.lower() in text.lower():\n                    text = text.lower().replace(wrong.lower(), correct)\n\n        return text\n\n    def correct_manipulation_terms(self, text):\n        \"\"\"\n        Correct common manipulation-related misrecognitions\n        \"\"\"\n        corrections = {\n            'cup': ['cup', 'cop', 'cap'],\n            'box': ['box', 'bogs', 'books'],\n            'bottle': ['bottle', 'battle', 'bodle']\n        }\n\n        for correct, possible in corrections.items():\n            for wrong in possible:\n                if wrong.lower() in text.lower():\n                    text = text.lower().replace(wrong.lower(), correct)\n\n        return text\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-robotics-systems",children:"Integration with Robotics Systems"}),"\n",(0,o.jsx)(n.h3,{id:"robot-speech-interface",children:"Robot Speech Interface"}),"\n",(0,o.jsx)(n.p,{children:"Integrating speech recognition with robot control:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class RobotSpeechInterface:\n    def __init__(self):\n        self.speech_recognizer = StreamingSpeechRecognizer(model_path='model.pt')\n        self.nlu_system = ContextAwareNLU()\n        self.robot_controller = RobotController()  # Assumed implementation\n        self.response_generator = ResponseGenerator()  # Assumed implementation\n\n    def process_speech_command(self, audio_input, robot_state):\n        \"\"\"\n        Process speech command and execute robot action\n        \"\"\"\n        # Recognize speech\n        recognition_result = self.speech_recognizer.recognize(audio_input)\n\n        # Understand intent and entities\n        context = {\n            'robot_state': robot_state,\n            'current_task': self.get_current_task(),\n            'available_objects': self.get_visible_objects(),\n            'known_locations': self.get_known_locations()\n        }\n\n        nlu_result = self.nlu_system.process_utterance(\n            recognition_result['text'], context\n        )\n\n        # Validate action safety\n        if self.is_safe_action(nlu_result):\n            # Execute action\n            execution_result = self.execute_robot_action(nlu_result)\n\n            # Generate response\n            response = self.response_generator.generate_response(\n                nlu_result, execution_result\n            )\n\n            return {\n                'success': True,\n                'action': nlu_result,\n                'response': response,\n                'execution_result': execution_result\n            }\n        else:\n            # Generate safety warning response\n            safety_response = self.response_generator.generate_safety_response(nlu_result)\n            return {\n                'success': False,\n                'action': nlu_result,\n                'response': safety_response,\n                'error': 'Action not safe to execute'\n            }\n\n    def is_safe_action(self, action):\n        \"\"\"\n        Check if robot action is safe to execute\n        \"\"\"\n        # Check various safety constraints\n        intent = action['intent']\n        entities = action['entities']\n\n        if intent == 'navigation':\n            target_location = entities[0]['value'] if entities else None\n            return self.is_safe_navigation(target_location)\n\n        elif intent == 'manipulation':\n            target_object = entities[0]['value'] if entities else None\n            return self.is_safe_manipulation(target_object)\n\n        return True  # Default to safe for other actions\n\n    def execute_robot_action(self, action):\n        \"\"\"\n        Execute robot action based on NLU result\n        \"\"\"\n        intent = action['intent']\n        entities = action['entities']\n\n        if intent == 'navigation':\n            target = entities[0]['value'] if entities else None\n            return self.robot_controller.navigate_to(target)\n\n        elif intent == 'manipulation':\n            target = entities[0]['value'] if entities else None\n            return self.robot_controller.manipulate_object(target)\n\n        elif intent == 'speak':\n            text = entities[0]['value'] if entities else action['original_text']\n            return self.robot_controller.speak(text)\n\n        else:\n            # Default action - speak response\n            response = self.response_generator.generate_default_response(action)\n            return self.robot_controller.speak(response)\n\n    def start_continuous_listening(self):\n        \"\"\"\n        Start continuous speech recognition\n        \"\"\"\n        self.speech_recognizer.start_listening()\n\n        while True:\n            # Get speech input\n            audio = self.speech_recognizer.get_audio_input()\n\n            # Process if speech detected\n            if self.speech_recognizer.is_speech_detected(audio):\n                result = self.process_speech_command(audio, self.robot_controller.get_state())\n\n                # Execute response\n                self.robot_controller.execute_response(result['response'])\n\n    def get_current_task(self):\n        \"\"\"\n        Get the robot's current task\n        \"\"\"\n        return self.robot_controller.get_current_task()\n\n    def get_visible_objects(self):\n        \"\"\"\n        Get objects currently visible to the robot\n        \"\"\"\n        return self.robot_controller.get_visible_objects()\n\n    def get_known_locations(self):\n        \"\"\"\n        Get locations known to the robot\n        \"\"\"\n        return self.robot_controller.get_known_locations()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this section, you should be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement speech recognition pipelines for robotic applications"}),"\n",(0,o.jsx)(n.li,{children:"Apply noise reduction and beamforming techniques"}),"\n",(0,o.jsx)(n.li,{children:"Perform natural language understanding with intent classification"}),"\n",(0,o.jsx)(n.li,{children:"Handle recognition errors and implement robustness techniques"}),"\n",(0,o.jsx)(n.li,{children:"Adapt speech recognition to specific robotic domains"}),"\n",(0,o.jsx)(n.li,{children:"Integrate speech recognition with robot control systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,o.jsx)(n.p,{children:"Refer to the following code examples in the textbook repository:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"docs/static/code-examples/capstone/humanoid_capstone_template.py"})," - Template for humanoid robot integration"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a streaming speech recognition system"}),"\n",(0,o.jsx)(n.li,{children:"Create an intent classification system for robot commands"}),"\n",(0,o.jsx)(n.li,{children:"Design noise reduction algorithms for multi-microphone arrays"}),"\n",(0,o.jsx)(n.li,{children:"Implement context-aware natural language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Develop error handling and recovery mechanisms for speech recognition"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);