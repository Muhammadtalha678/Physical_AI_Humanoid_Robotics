"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5117],{5379:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"week-11-12/human-robot-interaction","title":"Natural Human-Robot Interaction Design","description":"Human-robot interaction (HRI) is crucial for humanoid robots that work alongside humans. This section covers the design principles, modalities, and techniques for creating natural and intuitive interactions between humans and humanoid robots.","source":"@site/docs/week-11-12/human-robot-interaction.md","sourceDirName":"week-11-12","slug":"/week-11-12/human-robot-interaction","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-11-12/human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammadtalha678/Physical-AI-Humanoid-Robotics/edit/main/docs/docs/week-11-12/human-robot-interaction.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Natural Human-Robot Interaction Design"},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Grasping with Humanoid Hands","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-11-12/manipulation-grasping"},"next":{"title":"Integrating GPT Models for Conversational AI","permalink":"/Physical-AI-Humanoid-Robotics/docs/week-13/gpt-conversational-ai"}}');var i=t(4848),a=t(8453);const o={sidebar_position:4,title:"Natural Human-Robot Interaction Design"},r="Natural Human-Robot Interaction Design",l={},c=[{value:"Foundations of Human-Robot Interaction",id:"foundations-of-human-robot-interaction",level:2},{value:"Interaction Modalities",id:"interaction-modalities",level:3},{value:"Design Principles",id:"design-principles",level:3},{value:"Speech and Natural Language Processing",id:"speech-and-natural-language-processing",level:2},{value:"Speech Recognition",id:"speech-recognition",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Speech Synthesis",id:"speech-synthesis",level:3},{value:"Gesture Recognition and Production",id:"gesture-recognition-and-production",level:2},{value:"Gesture Recognition",id:"gesture-recognition",level:3},{value:"Gesture Production",id:"gesture-production",level:3},{value:"Facial Expressions and Emotion",id:"facial-expressions-and-emotion",level:2},{value:"Facial Expression Control",id:"facial-expression-control",level:3},{value:"Emotion Recognition",id:"emotion-recognition",level:3},{value:"Proxemics and Spatial Interaction",id:"proxemics-and-spatial-interaction",level:2},{value:"Personal Space Management",id:"personal-space-management",level:3},{value:"Navigation for Social Interaction",id:"navigation-for-social-interaction",level:3},{value:"Multimodal Interaction Fusion",id:"multimodal-interaction-fusion",level:2},{value:"Sensor Fusion for Interaction",id:"sensor-fusion-for-interaction",level:3},{value:"Social Norms and Etiquette",id:"social-norms-and-etiquette",level:2},{value:"Social Behavior Planning",id:"social-behavior-planning",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"natural-human-robot-interaction-design",children:"Natural Human-Robot Interaction Design"})}),"\n",(0,i.jsx)(n.p,{children:"Human-robot interaction (HRI) is crucial for humanoid robots that work alongside humans. This section covers the design principles, modalities, and techniques for creating natural and intuitive interactions between humans and humanoid robots."}),"\n",(0,i.jsx)(n.h2,{id:"foundations-of-human-robot-interaction",children:"Foundations of Human-Robot Interaction"}),"\n",(0,i.jsx)(n.h3,{id:"interaction-modalities",children:"Interaction Modalities"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots can interact with humans through multiple modalities:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech"}),": Natural language communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gestures"}),": Body language and hand movements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Facial expressions"}),": Emotional communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Proxemics"}),": Spatial relationships and personal space"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Touch"}),": Physical interaction and haptics"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,i.jsx)(n.p,{children:"Effective HRI design follows these principles:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Anthropomorphism"}),": Making robots relatable through human-like features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Predictability"}),": Users should be able to anticipate robot behavior"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Transparency"}),": Robot's intentions should be clear to users"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": All interactions should be physically and emotionally safe"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context awareness"}),": Robot should adapt to the situation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"speech-and-natural-language-processing",children:"Speech and Natural Language Processing"}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Implementing robust speech recognition for HRI:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\nimport numpy as np\n\nclass SpeechRecognitionSystem:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.command_keywords = {\n            'navigation': ['go to', 'navigate to', 'move to', 'walk to'],\n            'manipulation': ['pick up', 'grasp', 'take', 'get'],\n            'social': ['hello', 'hi', 'goodbye', 'thank you']\n        }\n\n    def recognize_speech(self, audio_source=None):\n        \"\"\"\n        Recognize speech from audio source\n        \"\"\"\n        if audio_source is None:\n            audio_source = self.microphone\n\n        try:\n            with audio_source as source:\n                # Adjust for ambient noise\n                self.recognizer.adjust_for_ambient_noise(source)\n\n                # Listen for speech\n                audio = self.recognizer.listen(source, timeout=5.0)\n\n                # Recognize speech\n                text = self.recognizer.recognize_google(audio)\n                return text.lower()\n\n        except sr.WaitTimeoutError:\n            return None\n        except sr.UnknownValueError:\n            return None\n        except sr.RequestError:\n            return None\n\n    def parse_command(self, speech_text):\n        \"\"\"\n        Parse recognized speech into robot commands\n        \"\"\"\n        if not speech_text:\n            return None\n\n        # Identify command type based on keywords\n        for cmd_type, keywords in self.command_keywords.items():\n            for keyword in keywords:\n                if keyword in speech_text:\n                    return {\n                        'type': cmd_type,\n                        'command': speech_text,\n                        'parsed': self.extract_command_details(speech_text, keyword)\n                    }\n\n        return {'type': 'unknown', 'command': speech_text}\n\n    def extract_command_details(self, text, keyword):\n        \"\"\"\n        Extract specific details from the command\n        \"\"\"\n        # Simple extraction - in practice this would use NLP\n        parts = text.split(keyword)\n        if len(parts) > 1:\n            target = parts[1].strip()\n            return {'target': target, 'action': keyword}\n        return {'target': '', 'action': keyword}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,i.jsx)(n.p,{children:"Understanding the intent behind human speech:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class NaturalLanguageUnderstanding:\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()  # Assumed implementation\n        self.entity_extractor = EntityExtractor()    # Assumed implementation\n        self.dialogue_manager = DialogueManager()    # Assumed implementation\n\n    def process_utterance(self, text):\n        \"\"\"\n        Process human utterance and extract meaning\n        \"\"\"\n        # Classify intent\n        intent = self.intent_classifier.classify(text)\n\n        # Extract entities (objects, locations, etc.)\n        entities = self.entity_extractor.extract(text)\n\n        # Update dialogue state\n        dialogue_state = self.dialogue_manager.update(intent, entities)\n\n        return {\n            'intent': intent,\n            'entities': entities,\n            'dialogue_state': dialogue_state\n        }\n\n    def generate_response(self, user_input, context):\n        \"\"\"\n        Generate appropriate robot response\n        \"\"\"\n        processed = self.process_utterance(user_input)\n\n        # Generate response based on intent and context\n        if processed['intent'] == 'greeting':\n            return self.generate_greeting_response()\n        elif processed['intent'] == 'navigation_request':\n            return self.generate_navigation_response(processed['entities'])\n        elif processed['intent'] == 'manipulation_request':\n            return self.generate_manipulation_response(processed['entities'])\n        else:\n            return self.generate_default_response()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"speech-synthesis",children:"Speech Synthesis"}),"\n",(0,i.jsx)(n.p,{children:"Generating natural-sounding speech for robot responses:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pyttsx3\nimport asyncio\n\nclass SpeechSynthesisSystem:\n    def __init__(self):\n        self.engine = pyttsx3.init()\n        self.setup_voice_parameters()\n\n    def setup_voice_parameters(self):\n        """\n        Configure voice characteristics\n        """\n        # Get available voices\n        voices = self.engine.getProperty(\'voices\')\n\n        # Select appropriate voice (typically the first one)\n        if voices:\n            self.engine.setProperty(\'voice\', voices[0].id)\n\n        # Set speech rate (words per minute)\n        self.engine.setProperty(\'rate\', 150)\n\n        # Set volume (0.0 to 1.0)\n        self.engine.setProperty(\'volume\', 0.8)\n\n    def speak(self, text, blocking=True):\n        """\n        Speak the given text\n        """\n        self.engine.say(text)\n        if blocking:\n            self.engine.runAndWait()\n        else:\n            self.engine.startLoop(False)  # Non-blocking\n\n    def speak_async(self, text):\n        """\n        Speak text asynchronously\n        """\n        def speak_thread():\n            self.engine.say(text)\n            self.engine.runAndWait()\n\n        import threading\n        thread = threading.Thread(target=speak_thread)\n        thread.start()\n        return thread\n'})}),"\n",(0,i.jsx)(n.h2,{id:"gesture-recognition-and-production",children:"Gesture Recognition and Production"}),"\n",(0,i.jsx)(n.h3,{id:"gesture-recognition",children:"Gesture Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Recognizing human gestures using computer vision:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom sklearn.svm import SVC\n\nclass GestureRecognitionSystem:\n    def __init__(self):\n        self.model = SVC(kernel=\'rbf\')\n        self.feature_extractor = FeatureExtractor()\n        self.trained = False\n\n    def extract_gesture_features(self, hand_positions):\n        """\n        Extract features from hand positions over time\n        """\n        # Calculate relative positions between key points\n        features = []\n\n        for i in range(len(hand_positions) - 1):\n            # Calculate velocity\n            velocity = hand_positions[i+1] - hand_positions[i]\n            features.extend(velocity)\n\n            # Calculate acceleration\n            if i > 0:\n                acceleration = velocity - (hand_positions[i] - hand_positions[i-1])\n                features.extend(acceleration)\n\n        # Add trajectory features\n        if len(hand_positions) > 1:\n            total_distance = np.sum([np.linalg.norm(hand_positions[j+1] - hand_positions[j])\n                                   for j in range(len(hand_positions) - 1)])\n            features.append(total_distance)\n\n        return np.array(features)\n\n    def recognize_gesture(self, hand_trajectory):\n        """\n        Recognize gesture from hand trajectory\n        """\n        if not self.trained:\n            return \'unknown\'\n\n        features = self.extract_gesture_features(hand_trajectory)\n        prediction = self.model.predict([features])\n\n        return prediction[0]\n\n    def train_recognizer(self, gesture_data):\n        """\n        Train the gesture recognizer\n        """\n        X = []\n        y = []\n\n        for gesture_name, trajectories in gesture_data.items():\n            for trajectory in trajectories:\n                features = self.extract_gesture_features(trajectory)\n                X.append(features)\n                y.append(gesture_name)\n\n        self.model.fit(X, y)\n        self.trained = True\n'})}),"\n",(0,i.jsx)(n.h3,{id:"gesture-production",children:"Gesture Production"}),"\n",(0,i.jsx)(n.p,{children:"Generating natural gestures for the humanoid robot:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class GestureProductionSystem:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.gesture_library = self.load_gesture_library()\n\n    def load_gesture_library(self):\n        """\n        Load predefined gestures\n        """\n        return {\n            \'pointing\': self.create_pointing_gesture,\n            \'waving\': self.create_waving_gesture,\n            \'beckoning\': self.create_beckoning_gesture,\n            \'nodding\': self.create_nodding_gesture,\n            \'shaking_head\': self.create_shaking_gesture\n        }\n\n    def create_pointing_gesture(self, target_position, duration=2.0):\n        """\n        Create a pointing gesture toward a target\n        """\n        # Calculate joint angles for pointing\n        current_pose = self.robot_model.get_current_pose()\n\n        # Calculate required arm configuration to point to target\n        arm_joints = self.calculate_pointing_joints(\n            current_pose, target_position\n        )\n\n        # Create trajectory\n        trajectory = self.create_smooth_trajectory(\n            current_pose[\'arm\'], arm_joints, duration\n        )\n\n        return trajectory\n\n    def create_waving_gesture(self, duration=3.0, num_waves=3):\n        """\n        Create a waving gesture\n        """\n        # Define wave pattern\n        base_pose = self.robot_model.get_arm_pose(\'right\')\n\n        wave_trajectories = []\n        for i in range(num_waves):\n            # Create up position\n            up_pose = base_pose.copy()\n            up_pose[2] += 0.1  # Move hand up\n\n            # Create wave position\n            wave_pose = up_pose.copy()\n            wave_pose[1] += 0.1 * (-1)**i  # Alternate left/right\n\n            wave_trajectories.extend([\n                (up_pose, duration/(num_waves*2)),\n                (wave_pose, duration/(num_waves*2))\n            ])\n\n        return wave_trajectories\n\n    def execute_gesture(self, gesture_name, **kwargs):\n        """\n        Execute a predefined gesture\n        """\n        if gesture_name in self.gesture_library:\n            trajectory = self.gesture_library[gesture_name](**kwargs)\n            self.robot_model.execute_trajectory(trajectory)\n        else:\n            raise ValueError(f"Unknown gesture: {gesture_name}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"facial-expressions-and-emotion",children:"Facial Expressions and Emotion"}),"\n",(0,i.jsx)(n.h3,{id:"facial-expression-control",children:"Facial Expression Control"}),"\n",(0,i.jsx)(n.p,{children:"Controlling the robot's facial expressions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class FacialExpressionSystem:\n    def __init__(self, face_model):\n        self.face_model = face_model\n        self.expression_params = {\n            'happy': {'eyebrows': 0.2, 'mouth': 0.8, 'eyes': 0.7},\n            'sad': {'eyebrows': -0.3, 'mouth': -0.5, 'eyes': 0.3},\n            'surprised': {'eyebrows': 0.9, 'mouth': 0.6, 'eyes': 0.9},\n            'angry': {'eyebrows': -0.6, 'mouth': -0.3, 'eyes': 0.8},\n            'neutral': {'eyebrows': 0.0, 'mouth': 0.0, 'eyes': 0.5}\n        }\n\n    def set_expression(self, expression_name, intensity=1.0):\n        \"\"\"\n        Set facial expression with given intensity\n        \"\"\"\n        if expression_name not in self.expression_params:\n            expression_name = 'neutral'\n\n        params = self.expression_params[expression_name]\n\n        # Apply parameters with intensity scaling\n        adjusted_params = {\n            key: value * intensity\n            for key, value in params.items()\n        }\n\n        self.face_model.set_parameters(adjusted_params)\n\n    def blend_expressions(self, expr1, expr2, blend_ratio):\n        \"\"\"\n        Blend between two expressions\n        \"\"\"\n        params1 = self.expression_params[expr1]\n        params2 = self.expression_params[expr2]\n\n        blended = {}\n        for key in params1:\n            blended[key] = params1[key] * (1 - blend_ratio) + params2[key] * blend_ratio\n\n        self.face_model.set_parameters(blended)\n\n    def animate_expression_transition(self, from_expr, to_expr, duration=1.0):\n        \"\"\"\n        Animate smooth transition between expressions\n        \"\"\"\n        import time\n\n        steps = int(duration * 50)  # 50 steps per second\n        for i in range(steps + 1):\n            blend_ratio = i / steps\n            self.blend_expressions(from_expr, to_expr, blend_ratio)\n            time.sleep(duration / steps)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"emotion-recognition",children:"Emotion Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Recognizing human emotions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class EmotionRecognitionSystem:\n    def __init__(self):\n        self.face_classifier = FaceClassifier()  # Assumed implementation\n        self.emotion_model = EmotionModel()      # Assumed implementation\n\n    def recognize_emotion(self, image):\n        """\n        Recognize emotion from facial expression\n        """\n        # Detect faces in image\n        faces = self.face_classifier.detect_faces(image)\n\n        emotions = []\n        for face in faces:\n            # Extract facial features\n            features = self.extract_facial_features(face)\n\n            # Classify emotion\n            emotion = self.emotion_model.classify(features)\n            emotions.append(emotion)\n\n        return emotions\n\n    def extract_facial_features(self, face_image):\n        """\n        Extract features relevant for emotion recognition\n        """\n        # Convert to grayscale\n        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n\n        # Detect facial landmarks\n        landmarks = self.detect_landmarks(gray)\n\n        # Calculate feature vectors based on landmark positions\n        features = []\n        for i in range(len(landmarks) - 1):\n            distance = np.linalg.norm(landmarks[i] - landmarks[i+1])\n            features.append(distance)\n\n        # Add ratios of distances (e.g., eye width to height)\n        # These ratios are more invariant to head pose\n        features.extend(self.calculate_ratios(landmarks))\n\n        return np.array(features)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"proxemics-and-spatial-interaction",children:"Proxemics and Spatial Interaction"}),"\n",(0,i.jsx)(n.h3,{id:"personal-space-management",children:"Personal Space Management"}),"\n",(0,i.jsx)(n.p,{children:"Managing appropriate distances and spatial relationships:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ProxemicsManager:\n    def __init__(self):\n        self.space_zones = {\n            'intimate': (0.0, 0.45),    # 0-18 inches\n            'personal': (0.45, 1.2),    # 18 inches-4 feet\n            'social': (1.2, 3.6),       # 4-12 feet\n            'public': (3.6, float('inf'))  # 12+ feet\n        }\n\n        self.current_interactions = {}\n\n    def determine_appropriate_distance(self, interaction_type, user_relationship):\n        \"\"\"\n        Determine appropriate distance based on interaction type\n        \"\"\"\n        if interaction_type == 'greeting':\n            return self.space_zones['personal'][0]\n        elif interaction_type == 'conversation':\n            return self.space_zones['personal'][0] * 1.2\n        elif interaction_type == 'task_collaboration':\n            return self.space_zones['personal'][0] * 0.8  # Closer for collaboration\n        elif interaction_type == 'presentation':\n            return self.space_zones['social'][0]\n        else:\n            default_distance = self.space_zones['personal'][0]\n\n        # Adjust based on user relationship\n        if user_relationship == 'stranger':\n            increase_factor = 1.2\n        elif user_relationship == 'acquaintance':\n            increase_factor = 1.0\n        elif user_relationship == 'friend':\n            decrease_factor = 0.8\n        elif user_relationship == 'family':\n            decrease_factor = 0.6\n        else:\n            increase_factor = 1.0\n\n        return default_distance * increase_factor\n\n    def monitor_personal_space(self, user_positions, robot_position):\n        \"\"\"\n        Monitor and respond to personal space violations\n        \"\"\"\n        responses = []\n\n        for user_id, user_pos in user_positions.items():\n            distance = np.linalg.norm(robot_position - user_pos)\n\n            # Determine which zone this distance falls into\n            zone = self.classify_distance_zone(distance)\n\n            if zone == 'intimate' and not self.is_expected_intimate_interaction(user_id):\n                # Too close - take appropriate action\n                response = self.handle_too_close(user_id, user_pos, robot_position)\n                responses.append(response)\n            elif zone == 'public' and self.is_engaged_with_user(user_id):\n                # Too far - move closer if appropriate\n                response = self.handle_too_far(user_id, user_pos, robot_position)\n                responses.append(response)\n\n        return responses\n\n    def classify_distance_zone(self, distance):\n        \"\"\"\n        Classify distance into appropriate zone\n        \"\"\"\n        for zone, (min_dist, max_dist) in self.space_zones.items():\n            if min_dist <= distance < max_dist:\n                return zone\n\n        return 'public'  # Default to public zone for very large distances\n"})}),"\n",(0,i.jsx)(n.h3,{id:"navigation-for-social-interaction",children:"Navigation for Social Interaction"}),"\n",(0,i.jsx)(n.p,{children:"Planning paths that consider social conventions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SocialNavigationSystem:\n    def __init__(self, base_navigation):\n        self.base_nav = base_navigation\n        self.proxemics_manager = ProxemicsManager()\n\n    def plan_socially_aware_path(self, start, goal, humans_in_environment):\n        """\n        Plan path that considers human presence and social norms\n        """\n        # Start with basic path planning\n        base_path = self.base_nav.plan_path(start, goal)\n\n        # Modify path to respect human space\n        socially_aware_path = self.add_social_constraints(\n            base_path, humans_in_environment\n        )\n\n        return socially_aware_path\n\n    def add_social_constraints(self, path, humans):\n        """\n        Add social constraints to path\n        """\n        modified_path = path.copy()\n\n        for i, waypoint in enumerate(path):\n            # Check distance to each human\n            for human_pos in humans:\n                distance = np.linalg.norm(waypoint - human_pos)\n\n                # If too close, adjust waypoint\n                if distance < self.proxemics_manager.space_zones[\'personal\'][0]:\n                    # Calculate direction away from human\n                    direction = waypoint - human_pos\n                    direction = direction / np.linalg.norm(direction)\n\n                    # Move waypoint away from human\n                    safe_distance = self.proxemics_manager.space_zones[\'personal\'][0] * 1.1\n                    modified_path[i] = human_pos + direction * safe_distance\n\n        return modified_path\n\n    def yield_to_humans(self, robot_pos, human_poses, velocities):\n        """\n        Yield to humans when appropriate\n        """\n        for i, human_pos in enumerate(human_poses):\n            distance = np.linalg.norm(robot_pos - human_pos)\n\n            if distance < 2.0:  # Within 2 meters\n                # Check if human is moving toward robot\n                if self.is_approaching(robot_pos, human_pos, velocities[i]):\n                    # Yield by slowing down or stopping\n                    return self.calculate_yield_behavior(human_pos)\n\n        return None  # No yielding needed\n\n    def is_approaching(self, robot_pos, human_pos, human_velocity):\n        """\n        Determine if human is approaching robot\n        """\n        # Calculate direction from human to robot\n        direction_to_robot = robot_pos - human_pos\n        direction_to_robot = direction_to_robot / np.linalg.norm(direction_to_robot)\n\n        # Check if human velocity is in direction of robot\n        approach_angle = np.arccos(\n            np.clip(np.dot(human_velocity, direction_to_robot), -1, 1)\n        )\n\n        return approach_angle < np.pi / 4  # Within 45 degrees\n'})}),"\n",(0,i.jsx)(n.h2,{id:"multimodal-interaction-fusion",children:"Multimodal Interaction Fusion"}),"\n",(0,i.jsx)(n.h3,{id:"sensor-fusion-for-interaction",children:"Sensor Fusion for Interaction"}),"\n",(0,i.jsx)(n.p,{children:"Combining multiple sensory inputs for better understanding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class MultimodalFusionSystem:\n    def __init__(self):\n        self.speech_recognizer = SpeechRecognitionSystem()\n        self.gesture_recognizer = GestureRecognitionSystem()\n        self.emotion_recognizer = EmotionRecognitionSystem()\n        self.fusion_model = FusionModel()  # Assumed implementation\n\n    def fuse_modalities(self, speech_input, gesture_input, visual_input):\n        \"\"\"\n        Fuse information from multiple modalities\n        \"\"\"\n        # Process each modality separately\n        speech_result = self.speech_recognizer.process(speech_input)\n        gesture_result = self.gesture_recognizer.process(gesture_input)\n        emotion_result = self.emotion_recognizer.process(visual_input)\n\n        # Combine results with confidence weighting\n        fused_result = self.fusion_model.combine(\n            speech_result, gesture_result, emotion_result\n        )\n\n        return fused_result\n\n    def handle_ambiguous_input(self, modality_results):\n        \"\"\"\n        Handle cases where modalities provide conflicting information\n        \"\"\"\n        # Check for conflicts between modalities\n        conflicts = self.detect_conflicts(modality_results)\n\n        if conflicts:\n            # Ask for clarification\n            clarification_request = self.generate_clarification_request(conflicts)\n            return clarification_request\n        else:\n            # Return fused result\n            return self.fusion_model.combine(modality_results)\n\n    def detect_conflicts(self, results):\n        \"\"\"\n        Detect conflicts between different modalities\n        \"\"\"\n        conflicts = []\n\n        # Example: speech says \"yes\" but head shake detected\n        if (results['speech']['intent'] == 'affirmative' and\n            results['gesture']['type'] == 'head_shake'):\n            conflicts.append(('speech', 'gesture', 'contradiction'))\n\n        # Example: happy speech with sad facial expression\n        if (results['speech']['sentiment'] == 'positive' and\n            results['visual']['emotion'] == 'sad'):\n            conflicts.append(('speech', 'visual', 'sentiment_mismatch'))\n\n        return conflicts\n"})}),"\n",(0,i.jsx)(n.h2,{id:"social-norms-and-etiquette",children:"Social Norms and Etiquette"}),"\n",(0,i.jsx)(n.h3,{id:"social-behavior-planning",children:"Social Behavior Planning"}),"\n",(0,i.jsx)(n.p,{children:"Implementing appropriate social behaviors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SocialBehaviorPlanner:\n    def __init__(self):\n        self.social_rules = self.load_social_rules()\n        self.cultural_adaptations = {}\n\n    def load_social_rules(self):\n        \"\"\"\n        Load basic social rules for interaction\n        \"\"\"\n        return {\n            'greeting_protocol': self.greeting_behavior,\n            'attention_management': self.attention_behavior,\n            'turn_taking': self.turn_taking_behavior,\n            'politeness_patterns': self.politeness_behavior\n        }\n\n    def greeting_behavior(self, user_approach, context):\n        \"\"\"\n        Appropriate greeting behavior based on context\n        \"\"\"\n        if self.is_first_encounter(user_approach):\n            return {\n                'action': 'greeting',\n                'expression': 'happy',\n                'gesture': 'waving',\n                'speech': 'Hello! Nice to meet you!'\n            }\n        elif self.is_familiar_user(user_approach):\n            return {\n                'action': 'acknowledgment',\n                'expression': 'friendly',\n                'gesture': 'nodding',\n                'speech': 'Hi again! How are you today?'\n            }\n        else:\n            return {\n                'action': 'acknowledgment',\n                'expression': 'neutral',\n                'gesture': 'greeting',\n                'speech': 'Hello!'\n            }\n\n    def attention_behavior(self, multiple_users):\n        \"\"\"\n        Manage attention between multiple users\n        \"\"\"\n        if len(multiple_users) == 1:\n            # Focus attention on single user\n            return self.focus_attention(multiple_users[0])\n        elif len(multiple_users) > 1:\n            # Use turn-taking or split attention\n            return self.distribute_attention(multiple_users)\n        else:\n            # No users present\n            return self.assume_waiting_posture()\n\n    def turn_taking_behavior(self, conversation_state):\n        \"\"\"\n        Manage turn-taking in conversations\n        \"\"\"\n        # Determine if it's the robot's turn to speak\n        if self.is_robot_turn(conversation_state):\n            return self.generate_response(conversation_state)\n        else:\n            # Wait for human to speak\n            return self.assume_listening_posture()\n\n    def politeness_behavior(self, request_type, user_status):\n        \"\"\"\n        Apply appropriate politeness patterns\n        \"\"\"\n        base_response = self.generate_base_response(request_type)\n\n        # Add politeness markers based on context\n        if user_status == 'elderly':\n            return self.add_formal_politeness(base_response)\n        elif user_status == 'child':\n            return self.add_child_friendly_politeness(base_response)\n        elif user_status == 'authority_figure':\n            return self.add_respectful_politeness(base_response)\n        else:\n            return self.add_standard_politeness(base_response)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this section, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design multimodal interaction systems for humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Implement speech recognition and natural language processing"}),"\n",(0,i.jsx)(n.li,{children:"Create gesture recognition and production systems"}),"\n",(0,i.jsx)(n.li,{children:"Control facial expressions and emotional responses"}),"\n",(0,i.jsx)(n.li,{children:"Manage spatial relationships and proxemics"}),"\n",(0,i.jsx)(n.li,{children:"Implement social behavior patterns and etiquette"}),"\n",(0,i.jsx)(n.li,{children:"Fuse multiple interaction modalities for robust interaction"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.p,{children:"Refer to the following code examples in the textbook repository:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"docs/static/code-examples/capstone/humanoid_capstone_template.py"})," - Template for humanoid robot integration"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a multimodal interaction system combining speech and gestures"}),"\n",(0,i.jsx)(n.li,{children:"Create a facial expression controller for emotional communication"}),"\n",(0,i.jsx)(n.li,{children:"Design a proxemics manager for personal space awareness"}),"\n",(0,i.jsx)(n.li,{children:"Implement a social navigation system that respects human space"}),"\n",(0,i.jsx)(n.li,{children:"Develop a dialogue manager for natural conversation"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);